{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ZZDU_ajpYu9F",
      "metadata": {
        "id": "ZZDU_ajpYu9F"
      },
      "source": [
        "# **Chains**\n",
        "\n",
        "chains are a fundamental concept that allows you to execute complex tasks in a structured and efficient way.\n",
        "\n",
        "## **Why Chains?**\n",
        "\n",
        "Chains are invaluable due to their capacity to effortlessly blend diverse components, shaping a singular and coherent application. Through the creation of chains, multiple elements can seamlessly come together. Imagine this scenario: a chain is crafted to take in user input, polish it using a PromptTemplate, and subsequently pass on this refined response to a large language model (LLM). This streamlined process not only simplifies but also enriches the overall functionality of the system. In essence, chains serve as the linchpin, seamlessly connecting different parts of the application and enhancing its capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "548b1c6c",
      "metadata": {
        "id": "548b1c6c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade langchain langchain_community langchain_aws python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oE8SwJDrv8Mx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "oE8SwJDrv8Mx",
        "outputId": "aa602fc3-fee8-4d78-8034-457c6ca0127f"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y numpy pandas\n",
        "!pip install --no-cache-dir numpy==1.26.4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40224a75",
      "metadata": {
        "id": "40224a75"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = os.getenv(\"AWS_DEFAULT_REGION\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3485d1af",
      "metadata": {
        "id": "3485d1af"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./content/employee.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4559e482",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "4559e482",
        "outputId": "e89ded26-abc4-4734-a583-47a441ceeafc"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PrxH7ELIZABK",
      "metadata": {
        "id": "PrxH7ELIZABK"
      },
      "source": [
        "# **LLM Chain** - **The simplest chain**\n",
        "\n",
        "The LLMChain is a foundational system that includes a PromptTemplate, an OpenAI model (such as a Large Language Model or a ChatModel), and optionally, an output parser. It operates by transforming input parameters using the PromptTemplate into a coherent prompt, which is then fed into the model. The resulting output is further refined and formatted into a usable form by the OutputParser, if provided. This structured approach ensures effective utilization of language models for various applications, enhancing their functionality and utility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201646f3",
      "metadata": {
        "id": "201646f3"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437ec9e0",
      "metadata": {
        "id": "437ec9e0"
      },
      "outputs": [],
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "llm = ChatBedrock(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18955fef",
      "metadata": {
        "id": "18955fef"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What are the tools that need to be learned to earn the {designation}?,provide me one tool\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fa1076c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fa1076c",
        "outputId": "cb27359f-d779-4450-a23f-99470b19e684"
      },
      "outputs": [],
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "786b37fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "786b37fb",
        "outputId": "1f68bd7f-7b1a-4385-ec59-89398907ad06"
      },
      "outputs": [],
      "source": [
        "designation = \"Devops Engineer\"\n",
        "chain.invoke(designation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cFmIwlFHZn1h",
      "metadata": {
        "id": "cFmIwlFHZn1h"
      },
      "source": [
        "# **SimpleSequentialChain**\n",
        "\n",
        "Simple Sequential Chains allow for a single input to undergo a series of coherent transformations, resulting in a refined output. This sequential approach ensures systematic and efficient handling of data, making it ideal for scenarios where a linear flow of information processing is essential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e6d036",
      "metadata": {
        "id": "16e6d036"
      },
      "outputs": [],
      "source": [
        "# SimpleSequentialChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb809100",
      "metadata": {
        "id": "eb809100"
      },
      "outputs": [],
      "source": [
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What are the tools that need to be learned to earn the {designation}?,provide me one tool\"\n",
        ")\n",
        "\n",
        "# Chain 1\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b61f827",
      "metadata": {
        "id": "1b61f827"
      },
      "outputs": [],
      "source": [
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following \\\n",
        "    tool:{tool_name}\"\n",
        ")\n",
        "# chain 2\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba6be59d",
      "metadata": {
        "id": "ba6be59d"
      },
      "outputs": [],
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                             verbose=True\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c07daae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "9c07daae",
        "outputId": "9e08b9bd-04a0-4792-931b-d48971bcdb04"
      },
      "outputs": [],
      "source": [
        "overall_simple_chain.invoke(designation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i1d_gTGOZ3ZZ",
      "metadata": {
        "id": "i1d_gTGOZ3ZZ"
      },
      "source": [
        "# **SequentialChain**\n",
        "\n",
        "A sequential chain is a chain that combines various individual chains, where the output of one chain serves as the input for the next in a continuous sequence. It operates by running a series of chains consecutively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb38a41",
      "metadata": {
        "id": "dcb38a41"
      },
      "outputs": [],
      "source": [
        "# SequentialChain\n",
        "from langchain.chains import SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42db3f34",
      "metadata": {
        "id": "42db3f34"
      },
      "outputs": [],
      "source": [
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following designation\"\n",
        "    \"\\n\\n{Designation}\"\n",
        ")\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key=\"Designation_description\"\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed023b4",
      "metadata": {
        "id": "eed023b4"
      },
      "outputs": [],
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following description in 1 sentence:\"\n",
        "    \"\\n\\n{Designation_description}\"\n",
        ")\n",
        "# chain 2: input= English_Review and output= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key=\"summary\"\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad04be0",
      "metadata": {
        "id": "5ad04be0"
      },
      "outputs": [],
      "source": [
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What are the promgramming language that need to be learned for this designation in one line:\\n\\n{Designation}\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"programming_language\"\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b0105f",
      "metadata": {
        "id": "13b0105f"
      },
      "outputs": [],
      "source": [
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a short follow up response to the following \"\n",
        "    \"Write a short summary about the programming language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {programming_language}\"\n",
        ")\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"followup_message\"\n",
        "                     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d175021",
      "metadata": {
        "id": "2d175021"
      },
      "outputs": [],
      "source": [
        "# overall_chain: input= Review\n",
        "# and output= English_Review,summary, followup_message\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Designation\"],\n",
        "    output_variables=[\"Designation_description\",\"programming_language\", \"summary\",\"followup_message\"],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c2fb50d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c2fb50d",
        "outputId": "bd5228c6-7dfc-4cf2-fdef-0ef27a2b00ea",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "designation = df.Designation[2]\n",
        "overall_chain.invoke(designation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F609fKrGaUsi",
      "metadata": {
        "id": "F609fKrGaUsi"
      },
      "source": [
        "# **Router Chain**\n",
        "\n",
        "The Router Chain is used for complicated tasks. If we have multiple subchains, each of which is specialized for a particular type of input, we could have a router chain that decides which subchain to pass the input to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20dfe853",
      "metadata": {
        "id": "20dfe853"
      },
      "outputs": [],
      "source": [
        "software_engineer_template = \"\"\"You are a highly skilled software engineer. \\\n",
        "You excel at addressing queries about programming and software development in a clear\n",
        "and straightforward manner. \\\n",
        "When faced with a challenge outside your expertise, you candidly \\\n",
        "acknowledge the gap in your knowledge. \\\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "database_administrator_template = \"\"\"You are a highly skilled database administrator.\\\n",
        "You excel at answering questions about databases in a clear and precise manner, \\\n",
        "making even complex topics accessible to anyone.\\\n",
        "When faced with a query or issue you're unfamiliar with, \\\n",
        "you candidly admit that you need to look into it further, valuing accuracy over conjecture.\\\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "data_scientist_template = \"\"\"You are a highly skilled data scientist.\\\n",
        "You excel at interpreting complex datasets and presenting insights in a \\\n",
        "straightforward and comprehensible manner.\\\n",
        "When confronted with a query beyond your expertise, you acknowledge \\\n",
        "your limitations and advocate for further research or consultation.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "Machine_learning_engineer_template = \"\"\" You are a highly skilled Machine Learning Engineer.\\\n",
        "You excel at answering questions about machine learning algorithms and models in a clear and succinct manner.\\\n",
        "When confronted with a query outside your expertise, you candidly admit that you don't have the answer. \\\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a53989f",
      "metadata": {
        "id": "8a53989f"
      },
      "outputs": [],
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"software_engineer\",\n",
        "        \"description\": \"Good for answering questions about software_engineer\",\n",
        "        \"prompt_template\": software_engineer_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"database_administrator\",\n",
        "        \"description\": \"Good for answering database_administrator questions\",\n",
        "        \"prompt_template\": database_administrator_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"data_scientist\",\n",
        "        \"description\": \"Good for answering data_scientist questions\",\n",
        "        \"prompt_template\": data_scientist_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Machine_learning_engineer\",\n",
        "        \"description\": \"Good for answering Machine_learning_engineer questions\",\n",
        "        \"prompt_template\": Machine_learning_engineer_template\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74205787",
      "metadata": {
        "id": "74205787"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89ff0e89",
      "metadata": {
        "id": "89ff0e89"
      },
      "outputs": [],
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaf2a7de",
      "metadata": {
        "id": "aaf2a7de"
      },
      "outputs": [],
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e1b47d3",
      "metadata": {
        "id": "5e1b47d3"
      },
      "outputs": [],
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = r\"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising \\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not \\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a4ab1dd",
      "metadata": {
        "id": "9a4ab1dd"
      },
      "outputs": [],
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "961447be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "961447be",
        "outputId": "92406507-35ee-4f84-ab98-d7da46c094a2"
      },
      "outputs": [],
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b441063f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "b441063f",
        "outputId": "9b964eb7-8978-4887-ea9d-b4dd9fb48b47"
      },
      "outputs": [],
      "source": [
        "chain.invoke(\"What are the various categories of software?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd97d517",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "dd97d517",
        "outputId": "dd936220-93d3-4104-bfad-6a81c7aef80b"
      },
      "outputs": [],
      "source": [
        "chain.invoke(\"What are the differences between supervised and unsupervised learning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UDDmuisndnbJ",
      "metadata": {
        "id": "UDDmuisndnbJ"
      },
      "source": [
        "# **Let's Do an Activity**\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "Practice using chains with language models and structured output parsing to handle complex tasks efficiently.\n",
        "\n",
        "## **Scenario**\n",
        "\n",
        "You are tasked with building a system to assist users in understanding various technical concepts related to software engineering roles. Your goal is to use different chains to process user queries, transform them using prompt templates, and extract specific information using output parsers.\n",
        "\n",
        "## **Steps**\n",
        "\n",
        "* Define a Prompt Template\n",
        "* Prepare Sample Queries\n",
        "* Implement LLM Chains\n",
        "* Output Parsing\n",
        "* Interactive Chain Execution\n",
        "* Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heO0ao4VehSq",
      "metadata": {
        "id": "heO0ao4VehSq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
